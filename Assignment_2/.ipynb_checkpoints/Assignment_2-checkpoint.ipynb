{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "import os, json\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "url = 'https://api.twitter.com/1.1/account/verify_credentials.json'\n",
    "auth = OAuth1('E5eeHHPrKm0N8qkHhH1BOkSu6', 'iLkXetVRI93Xt32KJfY44CUfau1BgH4BHSotaDLHloaykLOmTR',\n",
    "                  '744995423308374017-ic5GQxFdUxYjgpFJSbAQ1rRaJluz4nJ', 'hZl4AAiu62CalzpY8BeJfX2wJXezROk0WsEhms9KxXQ4w')\n",
    "requests.get(url, auth=auth)\n",
    "\n",
    "\n",
    "searchWord = input(\"Please enter a search term: \")\n",
    "filepath = searchWord.replace(\" \",\"\").lower() \n",
    "searchWord = searchWord.replace(\" \", \"%20\")\n",
    "print(\"This is the search term: \" + searchWord)\n",
    "print(\"Path:\" + filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "getTweet = 'https://api.twitter.com/1.1/search/tweets.json?q=%40'+searchWord+'&count=50'\n",
    "#print(requests.get(getTweet, auth=auth))\n",
    "#print(\"URL: \" + getTweet)\n",
    "#response = requests.get(getTweet, auth=auth)\n",
    "\n",
    "\n",
    "#print(data)\n",
    "#json_str = json.dumps(data)\n",
    "#data = json.loads(json_str)\n",
    "# Writing JSON data\n",
    "\n",
    "#if not os.path.exists(path):\n",
    "    #os.makedirs(path)\n",
    "\n",
    "def merge(query):\n",
    "    data = {}\n",
    "    while len(data) < 50:\n",
    "        response = requests.get(getTweet, auth = auth) \n",
    "        response = response.json()\n",
    "        for status in response['statuses']:\n",
    "            data[status['id']] = status \n",
    "        sleep(1)\n",
    "        print(len(data))\n",
    "    return data\n",
    "\n",
    "data = merge(getTweet)\n",
    "#datetime.now().strftime('%H-%M')+ datetime.datetime.now().strftime(\"%B\") + dattime.now().strftime('%d%Y')+'.json'\n",
    "with open(filepath+'.json', 'w') as f:\n",
    "     json.dump(data, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pacific Time (US & Canada)': 8, 'Central Time (US & Canada)': 3, 'Santiago': 1, 'Mexico City': 1, 'Amsterdam': 3, 'Madrid': 1, 'Bangkok': 4, 'Brasilia': 1, 'Kuala Lumpur': 1, 'Rome': 1, 'Jakarta': 13, 'Midway Island': 1}\n",
      "['Santiago', 'Mexico City', 'Madrid', 'Brasilia', 'Kuala Lumpur', 'Rome', 'Midway Island', 'Central Time (US & Canada)', 'Amsterdam', 'Bangkok', 'Pacific Time (US & Canada)', 'Jakarta']\n",
      "['Santiago', 'Mexico City', 'Madrid', 'Brasilia', 'Kuala Lumpur', 'Rome', 'Midway Island', 'Central Time (US & Canada)', 'Amsterdam', 'Bangkok']\n"
     ]
    }
   ],
   "source": [
    "#TimeZones\n",
    "with open(filepath+'.json', 'r') as fr:\n",
    "    data = json.load(fr)\n",
    "time_zone_all = {}\n",
    "for id in data:\n",
    "    user = data[id]['user']\n",
    "    if user['time_zone'] is not None: \n",
    "        if user['time_zone'] in time_zone_all:\n",
    "            time_zone_all[user['time_zone']] += 1\n",
    "        else:\n",
    "            time_zone_all[user['time_zone']] = 1\n",
    "\n",
    "print(time_zone_all)\n",
    "print(sorted(time_zone_all, key=lambda x: time_zone_all[x]))\n",
    "\n",
    "top10 = sorted(time_zone_all, key=lambda x: time_zone_all[x])\n",
    "print(top10[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#emoticons\n",
    "emoticons = []\n",
    "for id in data:\n",
    "    text = data[id]['text']\n",
    "    emoticons.append(text.split())    \n",
    "\n",
    "#print(emoticons)\n",
    "\n",
    "list_of_codes = ['U+1F601','U+1F602','U+1F603','U+1F604','U+1F605','U+1F606','U+1F607','U+1F6018','U+1F609','U+1F60A'] \n",
    "matching = []\n",
    "for emo in emoticons:\n",
    "    #for em in emo:\n",
    "     #   if \"hey\" in em:\n",
    "      #      matching.append[em]\n",
    "    #matching += [token for token in emo if token.startswith('\\U0001')]\n",
    "    matching += [token for token in emo if 'hey' in token]\n",
    "    \n",
    "print(matching)\n",
    "print(len(matching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜€\n"
     ]
    }
   ],
   "source": [
    "print(\"\\U0001F600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pre processing\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "\n",
    "tweets_tokenized = []\n",
    "for id in data:\n",
    "    tweet = data[id]['text']\n",
    "    tweets_tokenized.append(preprocess(tweet))\n",
    "\n",
    "\n",
    "print(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "count_all = Counter()\n",
    "for line in tweets_tokenized:\n",
    "    count_all.update(line)\n",
    "print(count_all.most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download()\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "for line in tweets_tokenized:\n",
    "    terms_stop = [term for term in line if term not in stop]\n",
    "print(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "mydate = datetime.datetime.now()\n",
    "mydate.strftime(\"%B\")\n",
    "\n",
    "print(mydate.strftime(\"%B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-89091fd08500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meverytweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_tokenized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meverytweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m#tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_tokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-89091fd08500>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(s, lowercase)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0memoticon_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-89091fd08500>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "    \n",
    "for everytweet in tweets_tokenized:\n",
    "    print(preprocess(everytweet))\n",
    "#tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(tweets_tokenized)\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
